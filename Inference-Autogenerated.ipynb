{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93d5fcb-a248-4a51-be37-a8d400bfec0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is an auto-generated notebook to perform batch inference on a Spark DataFrame using a selected model from the model registry. This feature is in preview, and we would greatly appreciate any feedback through this form: https://databricks.sjc1.qualtrics.com/jfe/form/SV_1H6Ovx38zgCKAR0.\n",
    "\n",
    "## Instructions:\n",
    "1. Run the notebook against a cluster with Databricks ML Runtime version 12.2.x-cpu, to best re-create the training environment.\n",
    "2. Add additional data processing on your loaded table to match the model schema if necessary (see the \"Define input and output\" section below).\n",
    "3. \"Run All\" the notebook.\n",
    "4. Note: If the `%pip` does not work for your model (i.e. it does not have a `requirements.txt` file logged), modify to use `%conda` if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc96044-64a6-4108-961f-3988a4627b75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Global SST Forecasting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adcac2a6-50a2-4c2c-b88b-8faf347a9852",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Environment Recreation\n",
    "Run the notebook against a cluster with Databricks ML Runtime version 12.2.x-cpu, to best re-create the training environment.. The cell below downloads the model artifacts associated with your model in the remote registry, which include `conda.yaml` and `requirements.txt` files. In this notebook, `pip` is used to reinstall dependencies by default.\n",
    "\n",
    "### (Optional) Conda Instructions\n",
    "Models logged with an MLflow client version earlier than 1.18.0 do not have a `requirements.txt` file. If you are using a Databricks ML runtime (versions 7.4-8.x), you can replace the `pip install` command below with the following lines to recreate your environment using `%conda` instead of `%pip`.\n",
    "```\n",
    "conda_yml = os.path.join(local_path, \"conda.yaml\")\n",
    "%conda env update -f $conda_yml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed51a07-824a-4ba3-bb62-2a1caf046a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "import os\n",
    "\n",
    "model_uri = f\"models:/{model_name}/Production\"\n",
    "local_path = ModelsArtifactRepository(model_uri).download_artifacts(\"\") # download model from remote registry\n",
    "\n",
    "requirements_path = os.path.join(local_path, \"requirements.txt\")\n",
    "if not os.path.exists(requirements_path):\n",
    "  dbutils.fs.put(\"file:\" + requirements_path, \"\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8853b913-dd38-4673-a319-c180fae9e38d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: mlflow in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from -r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.2.2)\nRequirement already satisfied: prophet==1.1.1 in /databricks/python3/lib/python3.9/site-packages (from -r /tmp/tmpthmdvoko/requirements.txt (line 2)) (1.1.1)\nRequirement already satisfied: cloudpickle==2.0.0 in /databricks/python3/lib/python3.9/site-packages (from -r /tmp/tmpthmdvoko/requirements.txt (line 3)) (2.0.0)\nRequirement already satisfied: databricks-automl-runtime==0.2.15 in /databricks/python3/lib/python3.9/site-packages (from -r /tmp/tmpthmdvoko/requirements.txt (line 4)) (0.2.15)\nRequirement already satisfied: tqdm>=4.36.1 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (4.64.0)\nRequirement already satisfied: wheel>=0.37.0 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (0.37.1)\nRequirement already satisfied: matplotlib>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (3.5.1)\nRequirement already satisfied: holidays>=0.14.2 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (0.18)\nRequirement already satisfied: numpy>=1.15.4 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (1.21.5)\nRequirement already satisfied: setuptools>=42 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (61.2.0)\nRequirement already satisfied: pandas>=1.0.4 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.0 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (2.8.2)\nRequirement already satisfied: setuptools-git>=1.2 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (1.2)\nRequirement already satisfied: cmdstanpy>=1.0.4 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (1.1.0)\nRequirement already satisfied: LunarCalendar>=0.0.9 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (0.0.9)\nRequirement already satisfied: convertdate>=2.1.2 in /databricks/python3/lib/python3.9/site-packages (from prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (2.4.0)\nRequirement already satisfied: gunicorn<21 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (20.1.0)\nRequirement already satisfied: docker<7,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (6.0)\nRequirement already satisfied: pyarrow<12,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (7.0.0)\nRequirement already satisfied: shap<1,>=0.40 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.41.0)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (3.1.27)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (3.19.4)\nRequirement already satisfied: pytz<2023 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2021.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.27.1)\nRequirement already satisfied: alembic<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.10.3)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.7.3)\nRequirement already satisfied: Flask<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.1.2)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.0.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.4)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (3.3.4)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (4.11.3)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.0.9)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.17.4)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (8.0.4)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.9/site-packages (from alembic<2->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from alembic<2->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (4.5.0)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /databricks/python3/lib/python3.9/site-packages (from convertdate>=2.1.2->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (0.5.12)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.6.0)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.26.9)\nRequirement already satisfied: websocket-client>=0.32.0 in /databricks/python3/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.58.0)\nRequirement already satisfied: itsdangerous>=0.24 in /databricks/python3/lib/python3.9/site-packages (from Flask<3->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.0.1)\nRequirement already satisfied: Werkzeug>=0.15 in /databricks/python3/lib/python3.9/site-packages (from Flask<3->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (4.0.10)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (5.0.0)\nRequirement already satisfied: hijri-converter in /databricks/python3/lib/python3.9/site-packages (from holidays>=0.14.2->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (2.2.4)\nRequirement already satisfied: korean-lunar-calendar in /databricks/python3/lib/python3.9/site-packages (from holidays>=0.14.2->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (0.3.1)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (3.7.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.0.1)\nRequirement already satisfied: ephem>=3.7.5.3 in /databricks/python3/lib/python3.9/site-packages (from LunarCalendar>=0.0.9->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (4.1.4)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib>=2.0.0->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib>=2.0.0->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib>=2.0.0->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib>=2.0.0->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib>=2.0.0->prophet==1.1.1->-r /tmp/tmpthmdvoko/requirements.txt (line 2)) (9.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (1.1.1)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.55.1)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.0.7)\nRequirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages (from sqlalchemy<3,>=1.4.0->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (2.0.2)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /databricks/python3/lib/python3.9/site-packages (from numba->shap<1,>=0.40->mlflow->-r /tmp/tmpthmdvoko/requirements.txt (line 1)) (0.38.0)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r $requirements_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a0a289-8917-4d55-bf90-e02e113c0fdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define input and output\n",
    "The table path assigned to`input_table_name` will be used for batch inference and the predictions will be saved to `output_table_path`. After the table has been loaded, you can perform additional data processing, such as renaming or removing columns, to ensure the model and table schema matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38d7517-3b00-4281-9f65-362a2adffea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# redefining key variables here because %pip and %conda restarts the Python interpreter\n",
    "model_name = \"Global SST Forecasting\"\n",
    "input_table_name = \"default.forecasting_dates\"\n",
    "output_table_path = \"/FileStore/batch-inference/Global SST Forecasting2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbb4008-324b-4ae0-99d9-dc009663f9a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load table as a Spark DataFrame\n",
    "table = spark.table(input_table_name)\n",
    "\n",
    "# optionally, perform additional data processing (may be necessary to conform the schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbe7d51-37b2-4a04-b159-8571bae06518",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load model and run inference\n",
    "**Note**: If the model does not return double values, override `result_type` to the desired type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f48a2e3-98d3-4811-a3da-4798cbbea38c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/06 01:48:28 INFO mlflow.pyfunc: This UDF will use conda to recreate the model's software environment for inference. This may take extra time during execution.\n2023/04/06 01:48:28 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n2023/04/06 01:48:29 INFO mlflow.utils.conda: Conda environment /local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919 already exists.\n2023/04/06 01:48:29 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /databricks/conda/bin/../etc/profile.d/conda.sh && conda activate mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919 1>&2 && python -c \"\"']'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "model_uri = f\"models:/{model_name}/Production\"\n",
    "\n",
    "# create spark user-defined function for model prediction\n",
    "predict = mlflow.pyfunc.spark_udf(spark, model_uri, result_type=\"double\", env_manager=\"conda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1f0134-c5eb-438b-a66e-6d0b5e5af79c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df = table.withColumn(\"prediction\", predict(struct(*table.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c136762-8680-4918-8192-d7a5e3c52c71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save predictions\n",
    "**The default output path on DBFS is accessible to everyone in this Workspace. If you want to limit access to the output you must change the path to a protected location.**\n",
    "The cell below will save the output table to the specified FileStore path. `datetime.now()` is appended to the path to prevent overwriting the table in the event that this notebook is run in a batch inference job. To overwrite existing tables at the path, replace the cell below with:\n",
    "```python\n",
    "output_df.write.mode(\"overwrite\").save(output_table_path)\n",
    "```\n",
    "\n",
    "### (Optional) Write predictions to Unity Catalog\n",
    "If you have access to any UC catalogs, you can also save predictions to UC by specifying a table in the format `<catalog>.<database>.<table>`.\n",
    "```python\n",
    "output_table = \"\" # Example: \"ml.batch-inference.Global SST Forecasting\"\n",
    "output_df.write.saveAsTable(output_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afb8fc7-3c23-4d9e-9ab0-4c214527fa08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (10.139.64.11 executor 1): org.apache.spark.api.python.PythonException: 'Exception: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})'. Full traceback below:\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1276, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1065, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1248, in batch_predict_fn\n    return client.invoke(pdf).get_predictions()\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/client.py\", line 75, in invoke\n    raise Exception(\nException: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3266)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3257)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3257)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1427)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1427)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1427)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3546)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3484)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3472)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:123)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:111)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:93)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:462)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:459)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3453)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3444)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4368)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4366)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4366)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3443)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:720)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:839)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: 'Exception: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})'. Full traceback below:\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1276, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1065, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1248, in batch_predict_fn\n    return client.invoke(pdf).get_predictions()\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/client.py\", line 75, in invoke\n    raise Exception(\nException: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (10.139.64.11 executor 1): org.apache.spark.api.python.PythonException: 'Exception: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})'. Full traceback below:\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1276, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1065, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1248, in batch_predict_fn\n    return client.invoke(pdf).get_predictions()\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/client.py\", line 75, in invoke\n    raise Exception(\nException: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3266)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3257)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3257)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1427)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1427)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1427)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3546)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3484)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3472)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:123)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:111)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:93)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:462)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:459)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3453)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3444)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4368)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4366)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4366)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3443)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:720)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:839)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: 'Exception: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})'. Full traceback below:\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1276, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1065, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1248, in batch_predict_fn\n    return client.invoke(pdf).get_predictions()\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-86650ef2-e4f7-44fc-b8f9-042a4668edad/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/client.py\", line 75, in invoke\n    raise Exception(\nException: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
       "errorSummary": "PythonException: 'Exception: Invocation failed (error code 400, response: {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\", line 276, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\\\", line 413, in predict\\n    return self._predict_fn(data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/mlflow/pyfunc/model.py\\\", line 305, in predict\\n    return self.python_model.predict(self.context, model_input)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 129, in predict\\n    predict_df = self.model().predict(test_df)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/databricks/automl_runtime/forecast/prophet/model.py\\\", line 82, in model\\n    return model_from_json(self._model_json)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 206, in model_from_json\\n    return model_from_dict(model_dict)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/prophet/serialize.py\\\", line 167, in model_from_dict\\n    df = pd.read_json(StringIO(model_dict[attribute]), typ='frame', orient='table', convert_dates=['ds'])\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 784, in read_json\\n    return json_reader.read()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 975, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1001, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1134, in parse\\n    self._parse()\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_json.py\\\", line 1344, in _parse\\n    self.obj = parse_table_schema(json, precise_float=self.precise_float)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/io/json/_table_schema.py\\\", line 370, in parse_table_schema\\n    df = df.astype(dtypes)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6305, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/generic.py\\\", line 6324, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 451, in astype\\n    return self.apply(\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/managers.py\\\", line 352, in apply\\n    applied = getattr(b, f)(**kwargs)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/internals/blocks.py\\\", line 511, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 242, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 187, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/dtypes/astype.py\\\", line 116, in _astype_nansafe\\n    return dta.astype(dtype, copy=False)._ndarray\\n  File \\\"/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-22b20-a42ff-bc69e-e/mlflow/envs/conda_envs/mlflow-1b098421568e9125cc2f8cd1845d5a310d657280-96931d73dff136435a7f4a8c768701951631d919/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py\\\", line 682, in astype\\n    raise TypeError(\\nTypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype. Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead.\\n\"})'. Full traceback below:",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2ddf95-71d4-4b30-bba6-b12e7d5397cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"long_prediction\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Inference-Autogenerated",
   "notebookOrigID": 3574308964977197,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
